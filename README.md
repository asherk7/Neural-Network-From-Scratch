# Neural Network from Scratch  

## Overview  
This repository contains a custom implementation of a neural network built entirely from scratch using Python. The goal of this project is to gain a deep understanding of the inner workings of neural networks, including:  
- Forward propagation  
- Backward propagation  
- Activation functions  
- Loss functions  
- Optimization techniques  

The network will be trained and evaluated on the **CIFAR-10 dataset**, a widely used benchmark for image classification tasks.  

---

## Features  
- Manual implementation of key neural network components:
  - Multiple layers (e.g. Dense, Convolutional, etc.)
  - Non-linear activation functions (e.g., ReLU, Softmax)
  - Loss functions (e.g., Cross-Entropy)
  - Gradient descent and backpropagation
- Training and evaluation pipeline for the CIFAR-10 dataset
- Visualization of learning metrics (e.g., loss and accuracy)

---

## Objectives  
1. Understand the mathematical foundations of neural networks.  
2. Implement a neural network without relying on high-level libraries like TensorFlow or PyTorch.  
3. Build intuition on how different components work together.  

---

## Prerequisites  
- **Python 3.8+**  
- Basic knowledge of linear algebra, calculus, and programming.  

---

## Installation  
1. Clone this repository:  
   ```bash
   git clone https://github.com/asherk7/neural-network-from-scratch.git
   cd neural-network-from-scratch
   ```
